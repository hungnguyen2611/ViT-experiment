{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EfficientFormer\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import torch\n",
    "from src.models.components.opp_finetune import EfficientFormerV2_finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model = EfficientFormerV2_finetuned()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"efficientformerV2_s1.pth\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model = model.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\nModule 'Embedding' has no attribute 'new_proj' :\n  File \"/home/ds/Documents/Xomad/lightning-hydra-template/scripts/../src/models/components/efficientformerv2.py\", line 333\n    def forward(self, x):\n        if self.light:\n            out = self.new_proj(x) + self.skip(x)\n                  ~~~~~~~~~~~~~ <--- HERE\n        elif self.asub:\n            out_conv = self.conv(x)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscript\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/jit/_script.py:1286\u001B[0m, in \u001B[0;36mscript\u001B[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001B[0m\n\u001B[1;32m   1284\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m   1285\u001B[0m     obj \u001B[38;5;241m=\u001B[39m call_prepare_scriptable_func(obj)\n\u001B[0;32m-> 1286\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_recursive\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_script_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1287\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_recursive\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfer_methods_to_compile\u001B[49m\n\u001B[1;32m   1288\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m   1291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m create_script_dict(obj)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/jit/_recursive.py:476\u001B[0m, in \u001B[0;36mcreate_script_module\u001B[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001B[0m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tracing:\n\u001B[1;32m    475\u001B[0m     AttributeTypeIsSupportedChecker()\u001B[38;5;241m.\u001B[39mcheck(nn_module)\n\u001B[0;32m--> 476\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcreate_script_module_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnn_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconcrete_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstubs_fn\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/jit/_recursive.py:538\u001B[0m, in \u001B[0;36mcreate_script_module_impl\u001B[0;34m(nn_module, concrete_type, stubs_fn)\u001B[0m\n\u001B[1;32m    535\u001B[0m     script_module\u001B[38;5;241m.\u001B[39m_concrete_type \u001B[38;5;241m=\u001B[39m concrete_type\n\u001B[1;32m    537\u001B[0m \u001B[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001B[39;00m\n\u001B[0;32m--> 538\u001B[0m script_module \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRecursiveScriptModule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_construct\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcpp_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_fn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;66;03m# Compile methods if necessary\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m concrete_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m concrete_type_store\u001B[38;5;241m.\u001B[39mmethods_compiled:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/jit/_script.py:615\u001B[0m, in \u001B[0;36mRecursiveScriptModule._construct\u001B[0;34m(cpp_module, init_fn)\u001B[0m\n\u001B[1;32m    602\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    603\u001B[0m \u001B[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001B[39;00m\n\u001B[1;32m    604\u001B[0m \u001B[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    612\u001B[0m \u001B[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001B[39;00m\n\u001B[1;32m    613\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    614\u001B[0m script_module \u001B[38;5;241m=\u001B[39m RecursiveScriptModule(cpp_module)\n\u001B[0;32m--> 615\u001B[0m \u001B[43minit_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscript_module\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    617\u001B[0m \u001B[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001B[39;00m\n\u001B[1;32m    618\u001B[0m \u001B[38;5;66;03m# custom implementations and flip the _initializing bit.\u001B[39;00m\n\u001B[1;32m    619\u001B[0m RecursiveScriptModule\u001B[38;5;241m.\u001B[39m_finalize_scriptmodule(script_module)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/jit/_recursive.py:516\u001B[0m, in \u001B[0;36mcreate_script_module_impl.<locals>.init_fn\u001B[0;34m(script_module)\u001B[0m\n\u001B[1;32m    513\u001B[0m     scripted \u001B[38;5;241m=\u001B[39m orig_value\n\u001B[1;32m    514\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    515\u001B[0m     \u001B[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001B[39;00m\n\u001B[0;32m--> 516\u001B[0m     scripted \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_script_module_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43morig_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msub_concrete_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstubs_fn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    518\u001B[0m cpp_module\u001B[38;5;241m.\u001B[39msetattr(name, scripted)\n\u001B[1;32m    519\u001B[0m script_module\u001B[38;5;241m.\u001B[39m_modules[name] \u001B[38;5;241m=\u001B[39m scripted\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/jit/_recursive.py:538\u001B[0m, in \u001B[0;36mcreate_script_module_impl\u001B[0;34m(nn_module, concrete_type, stubs_fn)\u001B[0m\n\u001B[1;32m    535\u001B[0m     script_module\u001B[38;5;241m.\u001B[39m_concrete_type \u001B[38;5;241m=\u001B[39m concrete_type\n\u001B[1;32m    537\u001B[0m \u001B[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001B[39;00m\n\u001B[0;32m--> 538\u001B[0m script_module \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRecursiveScriptModule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_construct\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcpp_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_fn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;66;03m# Compile methods if necessary\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m concrete_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m concrete_type_store\u001B[38;5;241m.\u001B[39mmethods_compiled:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/jit/_script.py:615\u001B[0m, in \u001B[0;36mRecursiveScriptModule._construct\u001B[0;34m(cpp_module, init_fn)\u001B[0m\n\u001B[1;32m    602\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    603\u001B[0m \u001B[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001B[39;00m\n\u001B[1;32m    604\u001B[0m \u001B[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    612\u001B[0m \u001B[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001B[39;00m\n\u001B[1;32m    613\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    614\u001B[0m script_module \u001B[38;5;241m=\u001B[39m RecursiveScriptModule(cpp_module)\n\u001B[0;32m--> 615\u001B[0m \u001B[43minit_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscript_module\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    617\u001B[0m \u001B[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001B[39;00m\n\u001B[1;32m    618\u001B[0m \u001B[38;5;66;03m# custom implementations and flip the _initializing bit.\u001B[39;00m\n\u001B[1;32m    619\u001B[0m RecursiveScriptModule\u001B[38;5;241m.\u001B[39m_finalize_scriptmodule(script_module)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/jit/_recursive.py:516\u001B[0m, in \u001B[0;36mcreate_script_module_impl.<locals>.init_fn\u001B[0;34m(script_module)\u001B[0m\n\u001B[1;32m    513\u001B[0m     scripted \u001B[38;5;241m=\u001B[39m orig_value\n\u001B[1;32m    514\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    515\u001B[0m     \u001B[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001B[39;00m\n\u001B[0;32m--> 516\u001B[0m     scripted \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_script_module_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43morig_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msub_concrete_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstubs_fn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    518\u001B[0m cpp_module\u001B[38;5;241m.\u001B[39msetattr(name, scripted)\n\u001B[1;32m    519\u001B[0m script_module\u001B[38;5;241m.\u001B[39m_modules[name] \u001B[38;5;241m=\u001B[39m scripted\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/jit/_recursive.py:538\u001B[0m, in \u001B[0;36mcreate_script_module_impl\u001B[0;34m(nn_module, concrete_type, stubs_fn)\u001B[0m\n\u001B[1;32m    535\u001B[0m     script_module\u001B[38;5;241m.\u001B[39m_concrete_type \u001B[38;5;241m=\u001B[39m concrete_type\n\u001B[1;32m    537\u001B[0m \u001B[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001B[39;00m\n\u001B[0;32m--> 538\u001B[0m script_module \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRecursiveScriptModule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_construct\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcpp_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_fn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;66;03m# Compile methods if necessary\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m concrete_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m concrete_type_store\u001B[38;5;241m.\u001B[39mmethods_compiled:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/jit/_script.py:615\u001B[0m, in \u001B[0;36mRecursiveScriptModule._construct\u001B[0;34m(cpp_module, init_fn)\u001B[0m\n\u001B[1;32m    602\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    603\u001B[0m \u001B[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001B[39;00m\n\u001B[1;32m    604\u001B[0m \u001B[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    612\u001B[0m \u001B[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001B[39;00m\n\u001B[1;32m    613\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    614\u001B[0m script_module \u001B[38;5;241m=\u001B[39m RecursiveScriptModule(cpp_module)\n\u001B[0;32m--> 615\u001B[0m \u001B[43minit_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscript_module\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    617\u001B[0m \u001B[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001B[39;00m\n\u001B[1;32m    618\u001B[0m \u001B[38;5;66;03m# custom implementations and flip the _initializing bit.\u001B[39;00m\n\u001B[1;32m    619\u001B[0m RecursiveScriptModule\u001B[38;5;241m.\u001B[39m_finalize_scriptmodule(script_module)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/jit/_recursive.py:516\u001B[0m, in \u001B[0;36mcreate_script_module_impl.<locals>.init_fn\u001B[0;34m(script_module)\u001B[0m\n\u001B[1;32m    513\u001B[0m     scripted \u001B[38;5;241m=\u001B[39m orig_value\n\u001B[1;32m    514\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    515\u001B[0m     \u001B[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001B[39;00m\n\u001B[0;32m--> 516\u001B[0m     scripted \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_script_module_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43morig_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msub_concrete_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstubs_fn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    518\u001B[0m cpp_module\u001B[38;5;241m.\u001B[39msetattr(name, scripted)\n\u001B[1;32m    519\u001B[0m script_module\u001B[38;5;241m.\u001B[39m_modules[name] \u001B[38;5;241m=\u001B[39m scripted\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/jit/_recursive.py:542\u001B[0m, in \u001B[0;36mcreate_script_module_impl\u001B[0;34m(nn_module, concrete_type, stubs_fn)\u001B[0m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;66;03m# Compile methods if necessary\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m concrete_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m concrete_type_store\u001B[38;5;241m.\u001B[39mmethods_compiled:\n\u001B[0;32m--> 542\u001B[0m     \u001B[43mcreate_methods_and_properties_from_stubs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconcrete_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod_stubs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproperty_stubs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    543\u001B[0m     \u001B[38;5;66;03m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001B[39;00m\n\u001B[1;32m    544\u001B[0m     \u001B[38;5;66;03m# If done before, hooks can overshadow methods that aren't exported.\u001B[39;00m\n\u001B[1;32m    545\u001B[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/jit/_recursive.py:393\u001B[0m, in \u001B[0;36mcreate_methods_and_properties_from_stubs\u001B[0;34m(concrete_type, method_stubs, property_stubs)\u001B[0m\n\u001B[1;32m    390\u001B[0m property_defs \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mdef_ \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m property_stubs]\n\u001B[1;32m    391\u001B[0m property_rcbs \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mresolution_callback \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m property_stubs]\n\u001B[0;32m--> 393\u001B[0m \u001B[43mconcrete_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_methods_and_properties\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproperty_defs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproperty_rcbs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod_defs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod_rcbs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod_defaults\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: \nModule 'Embedding' has no attribute 'new_proj' :\n  File \"/home/ds/Documents/Xomad/lightning-hydra-template/scripts/../src/models/components/efficientformerv2.py\", line 333\n    def forward(self, x):\n        if self.light:\n            out = self.new_proj(x) + self.skip(x)\n                  ~~~~~~~~~~~~~ <--- HERE\n        elif self.asub:\n            out_conv = self.conv(x)\n"
     ]
    }
   ],
   "source": [
    "torch.jit.script(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "# Input to the model\n",
    "x = torch.randn(batch_size, 3, 224, 224)\n",
    "torch_out = model(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"onnx/efficientformerV2_s1.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=13,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}},\n",
    "                  verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/base/network.4/network.4.7/token_mixer/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/base/network.4/network.4.7/token_mixer/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/base/network.4/network.4.8/token_mixer/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/base/network.4/network.4.8/token_mixer/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/base/network.5/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/base/network.5/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/base/network.6/network.6.4/token_mixer/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/base/network.6/network.6.4/token_mixer/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/base/network.6/network.6.5/token_mixer/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/base/network.6/network.6.5/token_mixer/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = 'onnx/efficientformerV2_s0.onnx'\n",
    "model_quant = 'onnx/efficientformerV2_s0_dynamic_quantized.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant, op_types_to_quantize=['Gemm', 'MatMul'], extra_options={\"MatMulConstBOnly\":True})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
